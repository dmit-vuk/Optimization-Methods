{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f3a6fa0",
   "metadata": {},
   "source": [
    "# Домашнее задание 1\n",
    "\n",
    "Это домашнее задание по материалам 1-4 недели семестра (1-4 лекции). Дедлайн по отправке - 23:59 14 апреля.\n",
    "\n",
    "- Домашнее задание выполняется в этом же Jupyter Notebook'e.\n",
    "\n",
    "- Файл необходимо переименовать: __Фамилия_Имя__ (без пробелов в начале и конце). Пример: __Иванов_Иван__.\n",
    "\n",
    "- ДЗ нужно отправлять на __OptimizationHomework@yandex.ru__. Тема письма: __МГУ_номер задания__ (без пробелов в начале и конце). Для данного ДЗ тема письма: __МГУ_1__.\n",
    "\n",
    "- Для решения можно использовать Google Colab, но присылать нужно не ссылку на Colab, а готовый notebook и все необходимые дополнительные файлы.\n",
    "\n",
    "- Решение каждой задачи/пункта задачи поместите после условия.\n",
    "\n",
    "- Не забывайте добавлять необходимые пояснения и комментарии.\n",
    "\n",
    "- В финальной версии, которая будет отправлена на проверку, должны быть удалены все отладочные артефакты. Под таким артефактами подразумеваются любые выводы ячеек, которые никак не прокоментированы в тексте, а также любой массовый/длинный технический вывод (даже если он прокомментирован в тексте).\n",
    "\n",
    "- При полном запуске решения (Kernel -> Restart & Run All) все ячейки должны выполняться без ошибок.\n",
    "\n",
    "- Суммарный балл за задание равен 240. Чтобы получить максимальный оценку за задание, нужно набрать 150 баллов. Баллы сверх 150 позволяют набрать оценку выше максимума.\n",
    "\n",
    "- Часть задач помечена $\\triangle$. Они также входят в максимальный балл за задание, но мы считаем, что достаточно выполнить задания без $\\triangle$, чтобы вникнуть в основные вещи, происходящие в соотвествующей части задания.\n",
    "\n",
    "Желаем успехов!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a386bbbb",
   "metadata": {},
   "source": [
    "### Часть 1. Дихотомия и Золотое сечение"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6267391f",
   "metadata": {},
   "source": [
    "__Определение__ Одномерная функция $f: [a,b] \\to \\mathbb{R}$ называется унимодальной на отрезке $[a,b]$, если существует $c^* \\in [a,b]$ такое, что \n",
    "\n",
    "1) для любых $a \\leq x < y \\leq c^*$ имеем $f(x) > f(y)$,\n",
    "\n",
    "2) для любых $c^* \\leq x < y \\leq b$ имеем $f(x) < f(y)$.\n",
    "\n",
    "В этом задании будем минимизировать такого рода функции.\n",
    "\n",
    "__Задача 1. (всего 40 баллов)__ Рассмотрим метод дихотомии:\n",
    "\n",
    "```python\n",
    "def binpoisk(f, a, b, eps):\n",
    "    c = (a + b) / 2\n",
    "    while (b - a) > eps:\n",
    "        d = (a + c) / 2\n",
    "        if f(d) <= f(c):\n",
    "            b = c\n",
    "            c = d\n",
    "        else:\n",
    "            e = (b + c) / 2\n",
    "            if f(c) <= f(e):\n",
    "                a = d\n",
    "                b = e\n",
    "            else:\n",
    "                a = c\n",
    "                c = e\n",
    "    return c\n",
    "```\n",
    "\n",
    "__а). (20 баллов)__ Докажите корректность для нахождения минимума унимодальной функции. Какова будет итерационная и оракульная сложность данного алгоритма для достижения точности $\\varepsilon$ (а именно, $|c_{out} - c^*| \\leq \\varepsilon$)? Т.е. необходимо получить верхнюю оценку на число итераций и подсчета значений $f$ метода дихотомии для минимизации унимодальных функций."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c8b5e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ваше решение (Markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e20a37a",
   "metadata": {},
   "source": [
    "__б). (5 баллов)__ Придумайте свою унимодальную функцию (постарайтесь придумать не самый тривиальный пример, если совсем не получается обратите внимание на функцию $f(x) = \\sin x \\cdot e^x$ и модернизируйте ее). Покажите, что она является унимодальной на каком то отрезке. Где у нее находится $c^*$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e4b0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ваше решение (Markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd87dc0",
   "metadata": {},
   "source": [
    "__в). (15 баллов)__ Реализуйте и запустите для этой функции метод дихотомии. Постройте 2 графика сходимости:\n",
    "\n",
    "1) по оси абцисс - число итераций, по оси ординат - $|c_{out} - c^*|$, \n",
    "\n",
    "2) по оси абцисс - число оракульных вызовов $f$, по оси ординат - $|c_{out} - c^*|$.\n",
    "\n",
    "Нанесите на оба графика теоретическую оценку из пункта а). Сделайте вывод.\n",
    "\n",
    "Следите за оформлением графиков: размер графика, масштаб осей (обычный или логарифмический), подписи осей (в том числе размер), легенда (так как на каждом графике будет две линии), толщина линий и т.д.. Графики должны быть удобны для чтения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1095502",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ваше решение (Code и Markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a06f3a",
   "metadata": {},
   "source": [
    "__Задача 2. (всего 20 баллов)__ Рассмотрим метод золотого сечения:\n",
    "\n",
    "```python\n",
    "def golden_ration(f, a, b, eps):\n",
    "    tau = (1 + sqrt(5)) / 2\n",
    "    while (b - a) > eps:\n",
    "        t = (b - a) / tau\n",
    "        c = b - t\n",
    "        d = a + t\n",
    "        if f(d) <= f(c):\n",
    "            a = c\n",
    "        else:\n",
    "            b = d\n",
    "    return (a + b) / 2\n",
    "```\n",
    "\n",
    "__а). (10 баллов)__ Докажите корректность и сходимость данного алгоритма для нахождения минимума унимодальной функции. Какова будет итерационная и оракульная сложность данного алгоритма для достижения точности $\\varepsilon$ (а именно, $|c_{out} - c^*| \\leq \\varepsilon$)? Лучше или хуже данный алгоритм, чем метод дихотомии?\n",
    "\n",
    "_Hint:_ возможно ли улучшения псевдокода метода золотого сечения с точки зрения оракульной сложности?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c05f060b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ваше решение (Markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65518ed1",
   "metadata": {},
   "source": [
    "__б). (10 баллов)__ Реализуйте метод золотого сечения. Добавьте результаты золотого сечения (теоретические и практические) на графики из пункта 1.в). Сделайте вывод."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcde785d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ваше решение (Code и Markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c74bce8",
   "metadata": {},
   "source": [
    "### Часть 2. Решаем задачу безусловной оптимизации\n",
    "\n",
    "Рассмотрим задачу минимизации эмпирического риска:\n",
    "\\begin{equation}\n",
    "\\min_{w \\in \\mathbb{R}^d} \\frac{1}{n} \\sum\\limits_{i=1}^n \\ell (g(w, x_i), y_i) + \\frac{\\lambda}{2} \\| w \\|^2_2,\n",
    "\\end{equation}\n",
    "где $\\ell$ - функция потерь, $g$ - модель, $w$ - параметры модели, $\\{x_i, y_i\\}_{i=1}^n$ - выборка данных из векторов признаков $x_i$ и меток $y_i$, $\\lambda > 0$ - параметр регуляризации.\n",
    "\n",
    "Используем линейную модель $g(w, x) = w^T x$ и логистическую/сигмоидную функцию потерь: $\\ell(z,y) = \\ln (1 + \\exp(-yz))$ (__Важно: $y$ должен принимать значения $-1$ или $1$__). Полученная задача называется задачей логистической регрессии. \n",
    "\n",
    "__Задача 1. (всего 25 баллов)__ Проведем подготовительную работу. \n",
    "\n",
    "__а). (15 баллов)__ Докажите, что градиент и гессиан для функции $f$ могут быть записаны в следующем виде:\n",
    "$$\n",
    "\\nabla f(w) = \\frac{1}{n} \\sum_{i=1}^n \\frac{-y_i}{1 + \\exp(y_i w^Tx_i)}x_i + \\lambda w,\n",
    "\\quad\n",
    "\\nabla^2 f(w) = \\frac{1}{n} \\sum_{i=1}^n \\frac{\\exp(y_i w^Tx_i)}{(1 + \\exp(y_i w^Tx_i))^2} x_i x_i^T + \\lambda I.\n",
    "$$\n",
    "\n",
    "Докажите, что $f$ является $\\mu$-сильно выпуклой и имеет $L$-Липшицев градиент, при этом $\\mu = \\lambda$, а $L = \\lambda + \\frac{1}{4n} \\sum_{i=1}^n \\| x_i\\|^2_2$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2857a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ваше решение (Markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1989322e",
   "metadata": {},
   "source": [
    "К заданию приложен датасет _mushrooms_. С помощью следующего кода сформируйте матрицу $X$ и вектор $y$, в которой и будет храниться выборка $\\{x_i, y_i\\}_{i=1}^n$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f48c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"mushrooms.txt\" \n",
    "#файл должен лежать в той же деректории, что и notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3d6d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_svmlight_file\n",
    "data = load_svmlight_file(dataset)\n",
    "X, y = data[0].toarray(), data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cb059b",
   "metadata": {},
   "source": [
    "Поменяем вектор $y$, чтобы $y_i$ принимали значения $-1$ и $1$. Вы также можете сделать дополнительную предобработку данных (приемами из машинного обучения), но это никак дополнительно не оценивается."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69a9245",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 2 * y - 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8587439",
   "metadata": {},
   "source": [
    "Разделим данные на две части: обучающую и тестовую."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61981ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04953da",
   "metadata": {},
   "source": [
    "__б). (10 баллов)__ Для обучающей части $X_{train}$, $y_{train}$ оцените константу $L$. Задайте $\\lambda$ так, чтобы $\\lambda \\approx L / 1000$.  Реализуйте в коде подсчет значения, градиента и гессиана для нашей целевой функции ($X$, $y$, $\\lambda$ лучше подавать в качестве параметра, чтобы была возможность их менять, а не только подставлять фиксированные $X_{train}$, $y_{train}$). Можно использовать как библиотеку ``numpy``, так и библиотеки ``autograd``, ``pytorch``, ``jax``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da8162b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ваше решение (Code и Markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f89e001",
   "metadata": {},
   "source": [
    "__Задача 2. (всего 70 баллов)__ Данная часть задания связана с градиентным спуском, моментумом и ускорением.\n",
    "\n",
    "__а). (10 балла)__ Реализуйте метод тяжелого шарика и ускоренный градиентный метод Нестерова. \n",
    "\n",
    "На всякий случай мы приводим здесь вариант описания функции для некоторого. Можно пользоваться таким форматом по желанию. Учтите, что в коде встречается ``x_sol`` - это проблему стоит как-то обойти или не использовать критерии, завязанные на ``x_sol``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd8fc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def method(f, nabla_f, x_0, x_sol, gamma_k,\n",
    "                     K = 10**3, eps = 10**-5, mode = 'x_k - x^*'):\n",
    "    '''\n",
    "        f - целевая функция\n",
    "        nabla_f - градиент целевой функции\n",
    "        x_0 - стартовая точка\n",
    "        x_sol - точное решение (оно нужно для подсчета ошибки)\n",
    "        gamma_k - функция для вычисления шага метода\n",
    "        K - количество итераций (по умолчанию 1е3)\n",
    "        eps - желаемая точность (по умолчанию 1е-5)\n",
    "        mode - критерий сходимости \n",
    "               Значения либо 'x_k - x^*' - тогда критерий сходимости будет ||x_k - x^*||,\n",
    "               либо 'f(x_k) - f(x^*)' - тогда критерий сходимости будет f(x_k) - f(x^*),\n",
    "               либо 'x_k+1 - x_k', либо 'f(x_k+1) - f(x_k)' (критерии будут аналогичными)\n",
    "\n",
    "        Функция возвращает точку, в которой достигается минимум и вектор ошибок\n",
    "    '''\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0574657f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ваше решение (Code и Markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6115bbd6",
   "metadata": {},
   "source": [
    "__б). (15 баллов)__ Решите задачу оптимизации на обучающей выборке с помощью метода градиентного спуска (тяжелый шарик с 0 моментумом). Попробуйте метод с разными шагами (но постоянными в одном запуске) $\\gamma_k = \\gamma$. Проверьте диапозон от $0$ до $3/L$. В данном пункте нужно построить следующий график: значения критерия сходимости от номера итерации градиетного спуска с различными значениями шага. Стартовую точку и критерий сходимости можете выбрать на свой вкус, мы советуем использовать нормированную версию критерия, например, $\\frac{\\| \\nabla f(x^k) \\|}{\\| \\nabla f(x^0) \\|}$, а также использовать далее в этой Задаче и в Задачах 3-4 ту же самую стартовую точку и тот же самый критерий сходимости. Сделайте вывод.\n",
    "\n",
    "Следите за оформлением графиков: размер графика, масштаб осей (обычный или логарифмический), подписи осей (в том числе размер), легенда (если на графике не одна линия), толщина линий и т.д.. Графики должны быть удобны для чтения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565ee28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ваше решение (Code и Markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112dea6d",
   "metadata": {},
   "source": [
    "__в). $\\triangle$ (30 баллов)__  Исследуйте другие техники подбора шага: \n",
    "\n",
    "1) Уменьшающийся с номером итерации шаг: $\\gamma_k = \\frac{\\gamma}{\\delta + k}$ или $\\gamma_k = \\frac{\\gamma}{\\delta + \\sqrt{k}}$, где $\\gamma$ и $\\delta$ нужно подобрать отдельно. Какая конфигурация $\\gamma$ и $\\delta$ показала наилучший результат?\n",
    "\n",
    "2) Наискорейший спуск: $\\gamma_k = \\arg\\min_{\\gamma} f(x_k - \\gamma \\nabla f(x_k))$. Как будете вычислять значение $\\gamma_k$ в данном случае?\n",
    "\n",
    "3) Шаг Поляка-Шора: $\\gamma_k = \\tfrac{f(x^k) - f(x^*)}{\\alpha \\|\\nabla f(x^k)\\|^2_2}$, где $\\alpha \\geq 1$ нужно подобрать отдельно. Какое $\\alpha$ дало лучший результат?\n",
    "\n",
    "Сравните все подходы подбора шага между собой. Постройте графики сравнения и сделайте вывод."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f45e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ваше решение (Code и Markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05b6540",
   "metadata": {},
   "source": [
    "__в). (15 баллов)__ Теперь решите задачу оптимизации на обучающей выборке с помощью метода тяжелого шарика и метода Нестерова. Зафиксируйте шаг $\\frac{1}{L}$ и перебирайте разные значения моментума от -1 до 1. Проверьте также значения моментума равные $\\frac{k}{k+3}$, $\\frac{k}{k+2}$, $\\frac{k}{k+1}$ ($k$ - номер итерации), $\\frac{\\sqrt{L} - \\sqrt{\\mu}}{\\sqrt{L} + \\sqrt{\\mu}}$.\n",
    "\n",
    "В данном пункте нужно построить три графика: 1) значения критерия сходимости от номера итерации для метода тяжелого шарика с различными значениями моментума, 2) значения критерия сходимости от номера итерации для ускоренного градиентного метода с различными значениями моментума, 3) значения критерия сходимости от номера итерации для двух методов с наилучшим выбором моментума для каждого, а также градиентного спуска.\n",
    "\n",
    "Не забывайте делать выводы и комментировать результаты. Например, отразите всегда ли сходимость является монотонной?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67303535",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ваше решение (Code и Markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e5fdc2",
   "metadata": {},
   "source": [
    "__Задача 3. (всего 65 баллов)__ Теперь поговорим про метод Ньютона и квазиньютоновские методы.\n",
    "\n",
    "__а). (20 баллов)__ Для задачи регресии реализуйте классический метод Ньютона и запустите его. Сходится ли он? Если нет, то попробуйте перед использованием метода Ньютона сначала запускать метод градиентного спуска на несколько итераций. Варьируйте количество шагов градиентного спуска. Постройте график значения критерия сходимости от номера итерации для комбинации градиентного спуска и метода Ньютона с различным числом шагов градиентного спуска. Сделайте вывод. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f15759",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ваше решение (Code и Markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf3b727",
   "metadata": {},
   "source": [
    "__б). (15 + 30 баллов)__ Для данной задачи реализуйте квазиньютоновский метод BFGS ($\\triangle$ можно реализовать более продвинутую версию L-BFGS, посмотрев оригинальную [статью](http://users.iems.northwestern.edu/~nocedal/PDFfiles/limited-memory.pdf) или лучше параграф 9.1 [книги](https://www.ime.unicamp.br/~pulino/MT404/TextosOnline/NocedalJ.pdf)). За реализацию L-BFGS и объяснение, как это сделать правильно и вычислительно эффективно, можно получить еще __30 баллов__. Используйте метод(ы) для решения задачи регресии. Добавьте его(их) на график из предыдущего пункта. Сделайте вывод."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fadb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ваше решение (Code и Markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225eedca",
   "metadata": {},
   "source": [
    "__Задача 4. (20 баллов)__ Осталось объеденить результаты полученные в Задачах 1-3. Для этого вспомним, что исходная задача регрессии является задачой машинного обучения и с помощью линейной модели $g$ можно предсказывать значения меток $y$. Как использовать итоговую модель для предсказания? Ответив на вопрос, сделайте предсказания на тестовой выборке $X_{test}$. Сравните с реальными метками $y_{test}$. Количество правильно угаданных меток есть точность/accuracy модели. Сравните метод градиентного спуска, метод тяжелого шарика, ускоренный градиентный метод, метод Ньютона, BFGS(L-BFGS). Постройте два графика: значение критерия сходимости от времени работы и точность предсказания от времени работы. Сделайте вывод."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa77b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ваше решение (Code и Markdown)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
